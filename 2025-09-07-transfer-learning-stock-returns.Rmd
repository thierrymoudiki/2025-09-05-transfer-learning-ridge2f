---
title: "2025-09-07 Transfer Learning using `ahead::ridge2f` on stocks returns"
author: "T. Moudiki"
date: "2025-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Contents**

- 0 - packages
- 0 - functions for generating synthetic stock returns
- 1 - pretraining `ahead::ridge2f` on synthetic stock returns
- 2 - out-of-sample benchmarking with GARCH models
- 3 - out-of-sample benchmarking with GARCH models (continued)

# 0 - packages

```{r 0-pkgs, message=FALSE, warning=FALSE}
# Just in case, uncomment to install packages
#install.packages("pak")
#pak::pak(c('forecast', "rugarch", 
#           "fGarch", 'quantmod', 
#           "zoo"))
#options(repos = c(techtonique = "https://r-packages.techtonique.net",
#                  CRAN = "https://cloud.r-project.org"))
#pak::pak(c('ahead', 'bayesianrvfl'))

require('ahead')
require('bayesianrvfl')
require('forecast')
require("rugarch")
require("fGarch")
require('quantmod')
require('zoo')

```

# 0 - functions for generating synthetic stock returns

```{r 0-functions}
library(forecast)

coverage_score <- function(obj, actual){
  if (is.null(obj$lower))
  {
    return(mean((obj$intervals[, 1] <= actual)*(actual <= obj$intervals[, 2]), na.rm=TRUE)*100)
  }
  return(mean((obj$lower <= actual)*(actual <= obj$upper), na.rm=TRUE)*100)
}
#coverage_score <- memoise::memoise(coverage_score)

winkler_score <- function(obj, actual, level = 95) {
  alpha <- 1 - level / 100
  lt <- try(obj$lower, silent = TRUE)
  ut <- try(obj$upper, silent = TRUE)
  actual <- as.numeric(actual)
  if (is.null(lt) || is.null(ut))
  {
    lt <- as.numeric(obj$intervals[, 1])
    ut <- as.numeric(obj$intervals[, 2])
  }
  n_points <- length(actual)
  stopifnot((n_points == length(lt)) && (n_points == length(ut)))
  diff_lt <- lt - actual
  diff_bounds <- ut - lt
  diff_ut <- actual - ut
  score <- diff_bounds
  score <- score + (2 / alpha) * (pmax(diff_lt, 0) + pmax(diff_ut, 0))
  return(mean(score, na.rm=TRUE))
}
#winkler_score <- memoise::memoise(winkler_score)

accuracy <- function(obj, actual, level = 95)
{
  actual <- as.numeric(actual)
  mean_prediction <- as.numeric(obj$mean)
  me <- mean(mean_prediction - actual)
  rmse <- sqrt(mean((mean_prediction - actual)**2, na.rm=TRUE))
  mae <- mean(abs(mean_prediction - actual), na.rm=TRUE)
  mpe <- mean(mean_prediction/actual-1, na.rm=TRUE)
  mape <- mean(abs(mean_prediction/actual-1), na.rm=TRUE)
  m <- frequency(obj$x)   # e.g., 12 for monthly data
  # Compute scaling factor (MAE of in-sample seasonal naive forecasts)
  if (m > 1) {
    scale <- mean(abs(diff(obj$x, lag = m)), na.rm=TRUE)
  } else {
    scale <- mean(abs(diff(obj$x, lag = 1)), na.rm=TRUE)
  }
  # MASE = mean(|test - forecast|) / scale
  mase <- mean(abs(actual - obj$mean), na.rm=TRUE) / max(scale, 1e-6)
  coverage <- as.numeric(coverage_score(obj, actual))
  winkler <- winkler_score(obj, actual, level = level)
  crps <- mean(scoringRules::crps_sample(y, dat))
  res <- c(me, rmse, mae, mpe,
           mape, mase, coverage, winkler, crps)
  names(res) <- c("me", "rmse", "mae", "mpe",
                  "mape", "mase", "coverage", "winkler", "crps")
  return(res)
}
#accuracy <- memoise::memoise(accuracy)


library(data.table)

# Main synthetic returns generator function
generate_synthetic_returns <- function(
    n_days = 252 * 10,
    mu = 0.0002,
    kappa = 0.05,
    theta = 0.0001,
    sigma_v = 0.01,
    rho = -0.7,
    lambda_jump = 0.05,
    jump_size_dist = "normal",
    sigma_jump = 0.02,
    noise_dist = "normal",
    noise_scale = 0.0005,
    noise_df = 5.0,
    regime_params = NULL,
    random_seed = NULL
) {

  if (!is.null(random_seed)) {
    set.seed(random_seed)
  }

  # Default regime switching parameters
  if (is.null(regime_params)) {
    regime_params <- list(
      transition_matrix = matrix(c(0.99, 0.01, 0.03, 0.97), nrow = 2, byrow = TRUE),
      theta_high_multiplier = 3.0,
      kappa_high_multiplier = 2.0
    )
  }

  # Validate transition matrix
  if (!is.null(regime_params$transition_matrix)) {
    row_sums <- rowSums(regime_params$transition_matrix)
    if (!all(abs(row_sums - 1) < 1e-6)) {
      stop("Transition matrix rows must sum to 1.")
    }
  }

  # Initialize arrays
  n <- n_days
  v <- numeric(n)
  r <- numeric(n)
  regime <- integer(n)

  # Initialize starting values
  v[1] <- theta
  regime[1] <- 0

  # Pre-generate all random numbers
  z_vol <- rnorm(n)
  z_return <- rnorm(n)
  jump_indicators <- rpois(n, lambda = lambda_jump)

  # 1. Simulate the Markov chain for regimes
  for (t in 2:n) {
    prev_regime <- regime[t-1] + 1
    probs <- regime_params$transition_matrix[prev_regime, ]
    regime[t] <- sample(0:1, size = 1, prob = probs)
  }

  # 2. Simulate the Heston process with jumps
  for (t in 2:n) {
    current_regime <- regime[t]
    if (current_regime == 1) {
      theta_t <- theta * regime_params$theta_high_multiplier
      kappa_t <- kappa * regime_params$kappa_high_multiplier
    } else {
      theta_t <- theta
      kappa_t <- kappa
    }

    # Robust volatility discretization
    v_prev <- v[t-1]
    eta <- z_vol[t]

    drift <- kappa_t * (theta_t - max(v_prev, 0))
    volvol_term <- sigma_v * sqrt(max(v_prev, 0)) * eta
    v_new <- v_prev + drift + volvol_term
    v[t] <- max(v_new, 0)

    # Return process
    epsilon_t <- rho * eta + sqrt(1 - rho^2) * z_return[t]
    diffusion_component <- sqrt(max(v_prev, 0)) * epsilon_t

    # Jump process (single jump per period)
    J <- 0
    if (jump_indicators[t] > 0) {
      if (jump_size_dist == "normal") {
        J <- rnorm(1, mean = 0, sd = sigma_jump)
      } else if (jump_size_dist == "log_normal") {
        log_J <- rnorm(1, mean = -0.5 * sigma_jump^2, sd = sigma_jump)
        J <- exp(log_J) - 1
      } else if (jump_size_dist == "exponential") {
        sign <- sample(c(-1, 1), 1)
        J <- sign * rexp(1, rate = 1/sigma_jump)
      } else {
        stop("Invalid jump_size_dist.")
      }
    }

    r[t] <- mu + diffusion_component + J
  }

  # 3. Add microstructure noise
  if (noise_dist == "normal") {
    noise <- rnorm(n, mean = 0, sd = noise_scale)
  } else if (noise_dist == "student_t") {
    if (noise_df <= 2) stop("noise_df must be > 2")
    scale_factor <- noise_scale / sqrt(noise_df / (noise_df - 2))
    noise <- rt(n, df = noise_df) * scale_factor
  } else {
    stop("Invalid noise_dist.")
  }
  r <- r + noise

  # 4. Create output data.table
  dt <- data.table(
    date = seq.Date(as.Date("1970-01-01"), by = "day", length.out = n),
    returns = r,
    variance = v,
    volatility = sqrt(v),
    regime = factor(regime, levels = c(0, 1), labels = c("Low Vol", "High Vol"))
  )

  return(dt)
}

# Wrapper function for generating diverse paths
generate_diverse_sv_paths <- function(
    n_paths = 10000,
    horizon = 252 * 5,
    frequency = "daily",
    jump_type = "mixed",
    include_regime_switching = TRUE,
    random_seed = NULL
) {

  if (!is.null(random_seed)) {
    set.seed(random_seed)
  }

  all_paths <- vector("list", n_paths)
  all_params <- vector("list", n_paths)

  for (i in 1:n_paths) {

    params <- list()

    # Sample diverse parameters
    params$mu <- runif(1, -0.0005, 0.0005)
    params$kappa <- runif(1, 0.02, 0.15)
    params$theta <- runif(1, 5e-5, 3e-4)
    params$sigma_v <- runif(1, 0.005, 0.03)
    params$rho <- runif(1, -0.85, -0.4)

    # Jump parameters
    params$lambda_jump <- sample(c(
      runif(1, 0.005, 0.02),
      runif(1, 0.02, 0.08),
      runif(1, 0.08, 0.15)
    ), 1)

    if (jump_type == "mixed") {
      params$jump_size_dist <- sample(
        c("normal", "log_normal", "exponential"),
        1,
        prob = c(0.4, 0.4, 0.2)
      )
    } else {
      params$jump_size_dist <- jump_type
    }

    params$sigma_jump <- runif(1, 0.01, 0.05)
    params$noise_dist <- sample(c("normal", "student_t"), 1, prob = c(0.7, 0.3))
    params$noise_scale <- runif(1, 1e-5, 2e-4)
    params$noise_df <- runif(1, 3, 8)

    # Regime switching parameters with FIXED transition matrices
    if (include_regime_switching) {
      regime_type <- sample(1:3, 1)

      if (regime_type == 1) {
        # Persistent regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.97, 0.995)
        p12 <- 1 - p11
        p21 <- runif(1, 0.01, 0.05)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      } else if (regime_type == 2) {
        # Mean-reverting regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.85, 0.92)
        p12 <- 1 - p11
        p21 <- runif(1, 0.08, 0.15)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      } else {
        # Rapid switching regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.7, 0.8)
        p12 <- 1 - p11
        p21 <- runif(1, 0.2, 0.3)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      }

      params$regime_params <- list(
        transition_matrix = transition_matrix,
        theta_high_multiplier = runif(1, 2.0, 5.0),
        kappa_high_multiplier = runif(1, 1.5, 3.0)
      )
    } else {
      params$regime_params <- NULL
    }

    # Generate the path
    path_data <- generate_synthetic_returns(
      n_days = horizon,
      mu = params$mu,
      kappa = params$kappa,
      theta = params$theta,
      sigma_v = params$sigma_v,
      rho = params$rho,
      lambda_jump = params$lambda_jump,
      jump_size_dist = params$jump_size_dist,
      sigma_jump = params$sigma_jump,
      noise_dist = params$noise_dist,
      noise_scale = params$noise_scale,
      noise_df = params$noise_df,
      regime_params = params$regime_params
    )

    all_paths[[i]] <- path_data$returns
    all_params[[i]] <- params

    if (i %% 1000 == 0) {
      message(sprintf("Generated %d/%d paths", i, n_paths))
    }
  }

  result <- list(
    paths = all_paths,
    parameters = all_params,
    horizon = horizon,
    frequency = frequency,
    n_paths = n_paths,
    generation_date = Sys.time()
  )

  class(result) <- "diverse_sv_paths"

  return(result)
}

# Summary method for the generated dataset
summary.diverse_sv_paths <- function(object, ...) {
  cat("Diverse Stochastic Volatility Paths Dataset\n")
  cat("===========================================\n")
  cat(sprintf("Number of paths: %d\n", object$n_paths))
  cat(sprintf("Horizon per path: %d days\n", object$horizon))
  cat(sprintf("Generation date: %s\n", object$generation_date))
  cat(sprintf("Total observations: %d\n", object$n_paths * object$horizon))

  # Sample statistics
  sample_paths <- sample(1:object$n_paths, min(100, object$n_paths))
  returns <- unlist(lapply(object$paths[sample_paths], function(x) x))

  cat("\nSummary statistics (sample):\n")
  cat(sprintf("Mean return: %.6f\n", mean(returns)))
  cat(sprintf("Return SD: %.6f\n", sd(returns)))
  cat(sprintf("Skewness: %.3f\n", moments::skewness(returns)))
  cat(sprintf("Kurtosis: %.3f\n", moments::kurtosis(returns)))
  cat(sprintf("Min return: %.6f\n", min(returns)))
  cat(sprintf("Max return: %.6f\n", max(returns)))
}
```

# 1 - pretraining `ahead::ridge2f` on synthetic stock returns

```{r 1-pretraining-ridge2, eval=FALSE}
# 2. Bayesian Optimization with rmse Loss ------------------------------------
set.seed(12345)
synthetic_data <- generate_diverse_sv_paths(n_paths = 1000, horizon = 500)
synthetic_paths <- synthetic_data$paths

# Objective function minimizing rmse with probabilistic forecasts
objective_function_rmse <- function(xx) {
  nb_hidden <- floor(xx[1])  
  lambda_1 <- 10^xx[3]
  lambda_2 <- 10^xx[4]
  lags_val <- min(floor(xx[2]), 100)  
  block_length <- floor(xx[5]) # this isn't tuned here
  centers <- floor(xx[6])
  rmse_scores <- rep(NA, length(synthetic_paths))
  
  pb <- utils::txtProgressBar(max = length(synthetic_paths), 
  style=3)
  
  for (i in seq_along(synthetic_paths)) {
    series <- synthetic_paths[[i]]
    train <- series[1:400]
    test <- series[401:500]    
    train_mean <- mean(train)
    train_sd <- sd(train)
    scaled_train <- (train - train_mean) / train_sd    
    # Generate probabilistic forecasts with block bootstrap
    fit <- try(ahead::ridge2f(
      y = scaled_train,
      h = length(test),
      lags = lags_val,
      nb_hidden = nb_hidden,
      lambda_1 = lambda_1,
      lambda_2 = lambda_2,
      centers = centers, 
      show_progress = FALSE
    ), silent = TRUE)    
    if (!inherits(fit, "try-error")) {
      # Rescale the predictive samples      
      predictive_samples <- fit$mean * train_sd + train_mean      
      # Calculate rmse for each forecast horizon and average      
      rmse_scores[i] <- sqrt(mean((test - predictive_samples)**2))
    }
    utils::setTxtProgressBar(pb, i)
  }
  close(pb)  
  median(rmse_scores, na.rm = TRUE)  # Minimize median rmse
}

# Run Bayesian optimization minimizing rmse
res_rmse <- bayesianrvfl::bayes_opt(
  objective_function_rmse,
  lower = c(3L, 1L, -4, -4, 5, 0), 
  upper = c(40L, 100L, 5, 5, 200, 5L),
  init_points = 10,
  n_iter = 50
)

saveRDS(res_rmse, "best_params_with_clustering.rds")
```

# 2 - out-of-sample benchmarking with GARCH models

```{r 2-out-of-sample-benchmarking}
# Install and load required packages
if (!require("rugarch")) install.packages("rugarch")
if (!require("fGarch")) install.packages("fGarch")
if (!require("forecast")) install.packages("forecast")
library(rugarch)
library(fGarch)
library(forecast)

best_params <- readRDS(file="best_params_with_clustering.rds")
print("Bayesian Optimization Parameters:")
print(best_params$best_param)

params <- list()
params$nb_hidden <- floor(best_params$best_param[1])
params$lags <- floor(best_params$best_param[2])
params$lambda_1 <- 10**best_params$best_param[3]
params$lambda_2 <- 10**best_params$best_param[4]
params$centers <- floor(best_params$best_param[6])
print("Transformed Parameters:")
print(params)

# Split data into train (90%) and test (10%)
n_series <- ncol(EuStockMarkets)
stock_data <- tail(EuStockMarkets, 501)
returns_stock_data <- diff(log(stock_data))
print(paste("Data dimensions:", dim(returns_stock_data)[1], "x", dim(returns_stock_data)[2]))

splitted_returns_stock_data <- misc::splitts(returns_stock_data, split_prob = 0.8)
n_test <- length(splitted_returns_stock_data$testing)

# Initialize comprehensive results storage
results <- list()

set.seed(123)

#pb <- utils::txtProgressBar(min = 0, max = ncol(EuStockMarkets), style = 3)

for (i in seq_len(ncol(EuStockMarkets))) {
  series_name <- colnames(EuStockMarkets)[i]
  train <- splitted_returns_stock_data$training[, i]
  test <- splitted_returns_stock_data$testing[, i]
  
  # Store forecasts for each method
  forecasts <- list()
  metrics <- list()
  
  # --- ridge2f (Transfer Learning Approach) ---
  train_mean <- mean(train)
  train_sd <- sd(train)
  scaled_train <- (train - train_mean) / train_sd
  
  fit_ridge <- try(ahead::ridge2f(
    y = scaled_train,
    h = length(test),
    nb_hidden = params$nb_hidden,
    lags = min(params$lags, length(scaled_train) - 1L),
    lambda_1 = params$lambda_1,
    lambda_2 = params$lambda_2,
    centers = params$centers,
    level = 95,
    B = 250,
    type_pi = "movingblockbootstrap",
    show_progress = FALSE, 
  ), silent = TRUE)
  
  if (!inherits(fit_ridge, "try-error")) {
    rescaled_mean <- fit_ridge$mean * train_sd + train_mean
    rescaled_lower <- fit_ridge$lower * train_sd + train_mean
    rescaled_upper <- fit_ridge$upper * train_sd + train_mean
    
    forecasts$ridge2 <- rescaled_mean
    
    # Calculate comprehensive metrics
    metrics$ridge2 <- list(
      winkler = misc::winkler_score(test, rescaled_lower, rescaled_upper, 95),
      coverage = mean((rescaled_lower <= test) & (test <= rescaled_upper)) * 100,
      interval_width = mean(rescaled_upper - rescaled_lower)
    )
  }
  
  # --- GARCH using rugarch ---
  garch_spec <- ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
    mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
    distribution.model = "norm"
  )
  
  garch_fit <- try(ugarchfit(
    spec = garch_spec,
    data = train,
    solver = "hybrid"
  ), silent = TRUE)
  
  if (!inherits(garch_fit, "try-error")) {
    garch_forecast <- ugarchforecast(garch_fit, n.ahead = length(test))
    garch_mean <- as.numeric(fitted(garch_forecast))
    garch_sigma <- as.numeric(sigma(garch_forecast))
    
    z_value <- qnorm(0.975)
    garch_lower <- garch_mean - z_value * garch_sigma
    garch_upper <- garch_mean + z_value * garch_sigma
    
    forecasts$rugarch <- garch_mean
    
    metrics$rugarch <- list(
      winkler = misc::winkler_score(test, garch_lower, garch_upper, 95),
      coverage = mean((garch_lower <= test) & (test <= garch_upper)) * 100,
      interval_width = mean(garch_upper - garch_lower)
    )
  }
  
  # --- GARCH using fGarch ---
  fgarch_fit <- try(garchFit(
    formula = ~ garch(1, 1),
    data = train,
    include.mean = FALSE,
    trace = FALSE,
    cond.dist = "norm"
  ), silent = TRUE)
  
  if (!inherits(fgarch_fit, "try-error")) {
    fgarch_forecast <- predict(fgarch_fit, n.ahead = length(test))
    fgarch_mean <- rep(0, length(test))  # fGarch assumes zero mean
    fgarch_sigma <- fgarch_forecast$standardDeviation
    
    z_value <- qnorm(0.975)
    fgarch_lower <- -z_value * fgarch_sigma
    fgarch_upper <- z_value * fgarch_sigma
    
    forecasts$fgarch <- fgarch_mean
    
    metrics$fgarch <- list(
      winkler = misc::winkler_score(test, fgarch_lower, fgarch_upper, 95),
      coverage = mean((fgarch_lower <= test) & (test <= fgarch_upper)) * 100,
      interval_width = mean(fgarch_upper - fgarch_lower)
    )
  }
  
  # Store results for this series
  results[[series_name]] <- list(forecasts = forecasts, metrics = metrics)
  
  #utils::setTxtProgressBar(pb, i)
}
#close(pb)

# Create comprehensive summary table
summary_table <- data.frame()
for (series_name in names(results)) {
  for (method in names(results[[series_name]]$metrics)) {
    metrics <- results[[series_name]]$metrics[[method]]
    summary_table <- rbind(summary_table, data.frame(
      Series = series_name,
      Method = method,
      Winkler = metrics$winkler,
      Coverage = metrics$coverage,
      Interval_Width = metrics$interval_width
    ))
  }
}

print(summary_table)



# Median performance across all series
avg_performance <- aggregate(. ~ Method, data = summary_table[, -1], median)
print("\n=== MEDIAN PERFORMANCE ACROSS ALL SERIES ===")
print(avg_performance)

# Statistical significance testing
cat("\n=== STATISTICAL COMPARISON ===\n")
methods <- unique(summary_table$Method)
for (metric in c("Winkler", "Coverage")) {
  cat(paste("\n", metric, "comparison:\n"))
  for (i in 1:(length(methods)-1)) {
    for (j in (i+1):length(methods)) {
      m1 <- summary_table[summary_table$Method == methods[i], metric]
      m2 <- summary_table[summary_table$Method == methods[j], metric]
      test_result <- t.test(m1, m2, paired = TRUE)
      cat(sprintf("%s vs %s: p-value = %.4f%s\n",
                  methods[i], methods[j], test_result$p.value,
                  ifelse(test_result$p.value < 0.05, " *", "")))
    }
  }
}
```

# 3 - out-of-sample benchmarking with GARCH models (continued)

```{r 3-additional-benchmark}
# Install and load necessary packages
library(quantmod)
library(zoo)

# List of ticker symbols for the CAC40 (example)
# You can replace this with the real tickers from Wikipedia or other sources
cac40_tickers <- c("AC.PA", "AI.PA", "AIR.PA", "ATO.PA", "BNP.PA", "CAP.PA", "CS.PA", "ENGI.PA", "GLE.PA", "KER.PA")

# Fetch the stock data for each ticker and calculate log-returns
log_returns_list <- list()
date_ <- as.Date("2025-09-05")

for (ticker in cac40_tickers) {
  # Get the stock data for the ticker
  getSymbols(ticker, src = "yahoo", from = date_ - 500, to = date_, auto.assign = TRUE)

  # Retrieve the Adjusted Close Prices
  stock_prices <- Cl(get(ticker))

  # Calculate log-returns
  log_returns <- diff(log(stock_prices))

  # Linear interpolation for missing data (weekends)
  log_returns_interpolated <- na.approx(log_returns)

  # Store the log-returns in the list
  log_returns_list[[ticker]] <- log_returns_interpolated
}

# Convert the list of log-returns into a data frame or matrix
log_returns_matrix <- do.call(cbind, log_returns_list)

# Convert the matrix to a ts object (time series)
log_returns_ts <- ts(log_returns_matrix, start = c(2023, 1), frequency = 252)  # Assuming 252 trading days per year

# View the first few rows of the log-returns time series
#head(log_returns_ts)

# Install and load required packages
if (!require("rugarch")) install.packages("rugarch")
if (!require("fGarch")) install.packages("fGarch")
if (!require("forecast")) install.packages("forecast")
library(rugarch)
library(fGarch)
library(forecast)

best_params <- readRDS(file="best_params_with_clustering.rds")
print("Bayesian Optimization Parameters:")
print(best_params$best_param)

params <- list()
params$nb_hidden <- floor(best_params$best_param[1])
params$lags <- floor(best_params$best_param[2])
params$lambda_1 <- 10**best_params$best_param[3]
params$lambda_2 <- 10**best_params$best_param[4]
params$centers <- floor(best_params$best_param[6])
print("Transformed Parameters:")
print(params)

# Split data into train (90%) and test (10%)
n_series <- ncol(log_returns_ts)
returns_stock_data <- log_returns_ts
print(paste("Data dimensions:", dim(returns_stock_data)[1], "x", dim(returns_stock_data)[2]))

splitted_returns_stock_data <- misc::splitts(returns_stock_data, split_prob = 0.8)
n_test <- length(splitted_returns_stock_data$testing)

# Initialize comprehensive results storage
results <- list()

set.seed(123)

pb <- utils::txtProgressBar(min = 0, max = ncol(log_returns_ts), style = 3)

for (i in seq_len(ncol(log_returns_ts))) {
  series_name <- colnames(log_returns_ts)[i]
  train <- splitted_returns_stock_data$training[, i]
  test <- splitted_returns_stock_data$testing[, i]

  # Store forecasts for each method
  forecasts <- list()
  metrics <- list()

  # --- ridge2f (Transfer Learning Approach) ---
  train_mean <- mean(train)
  train_sd <- sd(train)
  scaled_train <- (train - train_mean) / train_sd

  fit_ridge <- try(ahead::ridge2f(
    y = scaled_train,
    h = length(test),
    nb_hidden = params$nb_hidden,
    lags = min(params$lags, length(scaled_train) - 1L),
    lambda_1 = params$lambda_1,
    lambda_2 = params$lambda_2,
    centers = params$centers,
    level = 95,
    B = 250,
    type_pi = "movingblockbootstrap",
    show_progress = FALSE,
  ), silent = TRUE)

  if (!inherits(fit_ridge, "try-error")) {
    rescaled_mean <- fit_ridge$mean * train_sd + train_mean
    rescaled_lower <- fit_ridge$lower * train_sd + train_mean
    rescaled_upper <- fit_ridge$upper * train_sd + train_mean

    forecasts$ridge2 <- rescaled_mean

    # Calculate comprehensive metrics
    metrics$ridge2 <- list(
      winkler = misc::winkler_score(test, rescaled_lower, rescaled_upper, 95),
      coverage = mean((rescaled_lower <= test) & (test <= rescaled_upper)) * 100,
      interval_width = mean(rescaled_upper - rescaled_lower)
    )
  }

  # --- GARCH using rugarch ---
  garch_spec <- ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
    mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
    distribution.model = "norm"
  )

  garch_fit <- try(ugarchfit(
    spec = garch_spec,
    data = train,
    solver = "hybrid"
  ), silent = TRUE)

  if (!inherits(garch_fit, "try-error")) {
    garch_forecast <- ugarchforecast(garch_fit, n.ahead = length(test))
    garch_mean <- as.numeric(fitted(garch_forecast))
    garch_sigma <- as.numeric(sigma(garch_forecast))

    z_value <- qnorm(0.975)
    garch_lower <- garch_mean - z_value * garch_sigma
    garch_upper <- garch_mean + z_value * garch_sigma

    forecasts$rugarch <- garch_mean

    metrics$rugarch <- list(
      winkler = misc::winkler_score(test, garch_lower, garch_upper, 95),
      coverage = mean((garch_lower <= test) & (test <= garch_upper)) * 100,
      interval_width = mean(garch_upper - garch_lower)
    )
  }

  # --- GARCH using fGarch ---
  fgarch_fit <- try(garchFit(
    formula = ~ garch(1, 1),
    data = train,
    include.mean = FALSE,
    trace = FALSE,
    cond.dist = "norm"
  ), silent = TRUE)

  if (!inherits(fgarch_fit, "try-error")) {
    fgarch_forecast <- predict(fgarch_fit, n.ahead = length(test))
    fgarch_mean <- rep(0, length(test))  # fGarch assumes zero mean
    fgarch_sigma <- fgarch_forecast$standardDeviation

    z_value <- qnorm(0.975)
    fgarch_lower <- -z_value * fgarch_sigma
    fgarch_upper <- z_value * fgarch_sigma

    forecasts$fgarch <- fgarch_mean

    metrics$fgarch <- list(
      winkler = misc::winkler_score(test, fgarch_lower, fgarch_upper, 95),
      coverage = mean((fgarch_lower <= test) & (test <= fgarch_upper)) * 100,
      interval_width = mean(fgarch_upper - fgarch_lower)
    )
  }

  # Store results for this series
  results[[series_name]] <- list(forecasts = forecasts, metrics = metrics)

  utils::setTxtProgressBar(pb, i)
}
close(pb)

# Create comprehensive summary table
summary_table <- data.frame()
for (series_name in names(results)) {
  for (method in names(results[[series_name]]$metrics)) {
    metrics <- results[[series_name]]$metrics[[method]]
    summary_table <- rbind(summary_table, data.frame(
      Series = series_name,
      Method = method,
      Winkler = metrics$winkler,
      Coverage = metrics$coverage,
      Interval_Width = metrics$interval_width
    ))
  }
}

print(summary_table)



# Median performance across all series
avg_performance <- aggregate(. ~ Method, data = summary_table[, -1], median)
print("\n=== MEDIAN PERFORMANCE ACROSS ALL SERIES ===")
print(avg_performance)

# Statistical significance testing
cat("\n=== STATISTICAL COMPARISON ===\n")
methods <- unique(summary_table$Method)
for (metric in c("Winkler", "Coverage")) {
  cat(paste("\n", metric, "comparison:\n"))
  for (i in 1:(length(methods)-1)) {
    for (j in (i+1):length(methods)) {
      m1 <- summary_table[summary_table$Method == methods[i], metric]
      m2 <- summary_table[summary_table$Method == methods[j], metric]
      test_result <- t.test(m1, m2, paired = TRUE)
      cat(sprintf("%s vs %s: p-value = %.4f%s\n",
                  methods[i], methods[j], test_result$p.value,
                  ifelse(test_result$p.value < 0.05, " *", "")))
    }
  }
}
```

