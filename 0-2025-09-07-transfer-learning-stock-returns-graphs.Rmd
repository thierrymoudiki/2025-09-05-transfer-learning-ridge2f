---
title: "2025-09-09 Transfer Learning using `ahead::ridge2f` on stocks returns"
author: "T. Moudiki"
date: "2025-09-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Contents**

- 0 - packages
- 0 - functions for generating synthetic stock returns
- 1 - pretraining `ahead::ridge2f` on synthetic stock returns
- 2 - out-of-sample benchmarking with GARCH models
- 3 - out-of-sample benchmarking with GARCH models (continued)

# 0 - packages

```{r 0-pkgs, message=FALSE, warning=FALSE}
# Just in case, uncomment to install packages
#install.packages("pak")
#pak::pak(c('forecast', "rugarch", 
#           "fGarch", 'quantmod', 
#           "zoo"))
#options(repos = c(techtonique = "https://r-packages.techtonique.net",
#                  CRAN = "https://cloud.r-project.org"))
#pak::pak(c('ahead', 'bayesianrvfl'))

require('ahead')
require('bayesianrvfl')
require('forecast')
require("rugarch")
require("fGarch")
require('quantmod')
require('zoo')

```

# 0 - functions for generating synthetic stock returns

```{r 0-functions}
library(forecast)

coverage_score <- function(obj, actual){
  if (is.null(obj$lower))
  {
    return(mean((obj$intervals[, 1] <= actual)*(actual <= obj$intervals[, 2]), na.rm=TRUE)*100)
  }
  return(mean((obj$lower <= actual)*(actual <= obj$upper), na.rm=TRUE)*100)
}
#coverage_score <- memoise::memoise(coverage_score)

winkler_score <- function(obj, actual, level = 95) {
  alpha <- 1 - level / 100
  lt <- try(obj$lower, silent = TRUE)
  ut <- try(obj$upper, silent = TRUE)
  actual <- as.numeric(actual)
  if (is.null(lt) || is.null(ut))
  {
    lt <- as.numeric(obj$intervals[, 1])
    ut <- as.numeric(obj$intervals[, 2])
  }
  n_points <- length(actual)
  stopifnot((n_points == length(lt)) && (n_points == length(ut)))
  diff_lt <- lt - actual
  diff_bounds <- ut - lt
  diff_ut <- actual - ut
  score <- diff_bounds
  score <- score + (2 / alpha) * (pmax(diff_lt, 0) + pmax(diff_ut, 0))
  return(mean(score, na.rm=TRUE))
}
#winkler_score <- memoise::memoise(winkler_score)

accuracy <- function(obj, actual, level = 95)
{
  actual <- as.numeric(actual)
  mean_prediction <- as.numeric(obj$mean)
  me <- mean(mean_prediction - actual)
  rmse <- sqrt(mean((mean_prediction - actual)**2, na.rm=TRUE))
  mae <- mean(abs(mean_prediction - actual), na.rm=TRUE)
  mpe <- mean(mean_prediction/actual-1, na.rm=TRUE)
  mape <- mean(abs(mean_prediction/actual-1), na.rm=TRUE)
  m <- frequency(obj$x)   # e.g., 12 for monthly data
  # Compute scaling factor (MAE of in-sample seasonal naive forecasts)
  if (m > 1) {
    scale <- mean(abs(diff(obj$x, lag = m)), na.rm=TRUE)
  } else {
    scale <- mean(abs(diff(obj$x, lag = 1)), na.rm=TRUE)
  }
  # MASE = mean(|test - forecast|) / scale
  mase <- mean(abs(actual - obj$mean), na.rm=TRUE) / max(scale, 1e-6)
  coverage <- as.numeric(coverage_score(obj, actual))
  winkler <- winkler_score(obj, actual, level = level)
  crps <- mean(scoringRules::crps_sample(y, dat))
  res <- c(me, rmse, mae, mpe,
           mape, mase, coverage, winkler, crps)
  names(res) <- c("me", "rmse", "mae", "mpe",
                  "mape", "mase", "coverage", "winkler", "crps")
  return(res)
}
#accuracy <- memoise::memoise(accuracy)


library(data.table)

# Main synthetic returns generator function
generate_synthetic_returns <- function(
    n_days = 252 * 10,
    mu = 0.0002,
    kappa = 0.05,
    theta = 0.0001,
    sigma_v = 0.01,
    rho = -0.7,
    lambda_jump = 0.05,
    jump_size_dist = "normal",
    sigma_jump = 0.02,
    noise_dist = "normal",
    noise_scale = 0.0005,
    noise_df = 5.0,
    regime_params = NULL,
    random_seed = NULL
) {

  if (!is.null(random_seed)) {
    set.seed(random_seed)
  }

  # Default regime switching parameters
  if (is.null(regime_params)) {
    regime_params <- list(
      transition_matrix = matrix(c(0.99, 0.01, 0.03, 0.97), nrow = 2, byrow = TRUE),
      theta_high_multiplier = 3.0,
      kappa_high_multiplier = 2.0
    )
  }

  # Validate transition matrix
  if (!is.null(regime_params$transition_matrix)) {
    row_sums <- rowSums(regime_params$transition_matrix)
    if (!all(abs(row_sums - 1) < 1e-6)) {
      stop("Transition matrix rows must sum to 1.")
    }
  }

  # Initialize arrays
  n <- n_days
  v <- numeric(n)
  r <- numeric(n)
  regime <- integer(n)

  # Initialize starting values
  v[1] <- theta
  regime[1] <- 0

  # Pre-generate all random numbers
  z_vol <- rnorm(n)
  z_return <- rnorm(n)
  jump_indicators <- rpois(n, lambda = lambda_jump)

  # 1. Simulate the Markov chain for regimes
  for (t in 2:n) {
    prev_regime <- regime[t-1] + 1
    probs <- regime_params$transition_matrix[prev_regime, ]
    regime[t] <- sample(0:1, size = 1, prob = probs)
  }

  # 2. Simulate the Heston process with jumps
  for (t in 2:n) {
    current_regime <- regime[t]
    if (current_regime == 1) {
      theta_t <- theta * regime_params$theta_high_multiplier
      kappa_t <- kappa * regime_params$kappa_high_multiplier
    } else {
      theta_t <- theta
      kappa_t <- kappa
    }

    # Robust volatility discretization
    v_prev <- v[t-1]
    eta <- z_vol[t]

    drift <- kappa_t * (theta_t - max(v_prev, 0))
    volvol_term <- sigma_v * sqrt(max(v_prev, 0)) * eta
    v_new <- v_prev + drift + volvol_term
    v[t] <- max(v_new, 0)

    # Return process
    epsilon_t <- rho * eta + sqrt(1 - rho^2) * z_return[t]
    diffusion_component <- sqrt(max(v_prev, 0)) * epsilon_t

    # Jump process (single jump per period)
    J <- 0
    if (jump_indicators[t] > 0) {
      if (jump_size_dist == "normal") {
        J <- rnorm(1, mean = 0, sd = sigma_jump)
      } else if (jump_size_dist == "log_normal") {
        log_J <- rnorm(1, mean = -0.5 * sigma_jump^2, sd = sigma_jump)
        J <- exp(log_J) - 1
      } else if (jump_size_dist == "exponential") {
        sign <- sample(c(-1, 1), 1)
        J <- sign * rexp(1, rate = 1/sigma_jump)
      } else {
        stop("Invalid jump_size_dist.")
      }
    }

    r[t] <- mu + diffusion_component + J
  }

  # 3. Add microstructure noise
  if (noise_dist == "normal") {
    noise <- rnorm(n, mean = 0, sd = noise_scale)
  } else if (noise_dist == "student_t") {
    if (noise_df <= 2) stop("noise_df must be > 2")
    scale_factor <- noise_scale / sqrt(noise_df / (noise_df - 2))
    noise <- rt(n, df = noise_df) * scale_factor
  } else {
    stop("Invalid noise_dist.")
  }
  r <- r + noise

  # 4. Create output data.table
  dt <- data.table(
    date = seq.Date(as.Date("1970-01-01"), by = "day", length.out = n),
    returns = r,
    variance = v,
    volatility = sqrt(v),
    regime = factor(regime, levels = c(0, 1), labels = c("Low Vol", "High Vol"))
  )

  return(dt)
}

# Wrapper function for generating diverse paths
generate_diverse_sv_paths <- function(
    n_paths = 10000,
    horizon = 252 * 5,
    frequency = "daily",
    jump_type = "mixed",
    include_regime_switching = TRUE,
    random_seed = NULL
) {

  if (!is.null(random_seed)) {
    set.seed(random_seed)
  }

  all_paths <- vector("list", n_paths)
  all_params <- vector("list", n_paths)

  for (i in 1:n_paths) {

    params <- list()

    # Sample diverse parameters
    params$mu <- runif(1, -0.0005, 0.0005)
    params$kappa <- runif(1, 0.02, 0.15)
    params$theta <- runif(1, 5e-5, 3e-4)
    params$sigma_v <- runif(1, 0.005, 0.03)
    params$rho <- runif(1, -0.85, -0.4)

    # Jump parameters
    params$lambda_jump <- sample(c(
      runif(1, 0.005, 0.02),
      runif(1, 0.02, 0.08),
      runif(1, 0.08, 0.15)
    ), 1)

    if (jump_type == "mixed") {
      params$jump_size_dist <- sample(
        c("normal", "log_normal", "exponential"),
        1,
        prob = c(0.4, 0.4, 0.2)
      )
    } else {
      params$jump_size_dist <- jump_type
    }

    params$sigma_jump <- runif(1, 0.01, 0.05)
    params$noise_dist <- sample(c("normal", "student_t"), 1, prob = c(0.7, 0.3))
    params$noise_scale <- runif(1, 1e-5, 2e-4)
    params$noise_df <- runif(1, 3, 8)

    # Regime switching parameters with FIXED transition matrices
    if (include_regime_switching) {
      regime_type <- sample(1:3, 1)

      if (regime_type == 1) {
        # Persistent regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.97, 0.995)
        p12 <- 1 - p11
        p21 <- runif(1, 0.01, 0.05)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      } else if (regime_type == 2) {
        # Mean-reverting regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.85, 0.92)
        p12 <- 1 - p11
        p21 <- runif(1, 0.08, 0.15)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      } else {
        # Rapid switching regimes - FIXED: ensure rows sum to 1
        p11 <- runif(1, 0.7, 0.8)
        p12 <- 1 - p11
        p21 <- runif(1, 0.2, 0.3)
        p22 <- 1 - p21
        transition_matrix <- matrix(c(p11, p12, p21, p22), nrow = 2, byrow = TRUE)
      }

      params$regime_params <- list(
        transition_matrix = transition_matrix,
        theta_high_multiplier = runif(1, 2.0, 5.0),
        kappa_high_multiplier = runif(1, 1.5, 3.0)
      )
    } else {
      params$regime_params <- NULL
    }

    # Generate the path
    path_data <- generate_synthetic_returns(
      n_days = horizon,
      mu = params$mu,
      kappa = params$kappa,
      theta = params$theta,
      sigma_v = params$sigma_v,
      rho = params$rho,
      lambda_jump = params$lambda_jump,
      jump_size_dist = params$jump_size_dist,
      sigma_jump = params$sigma_jump,
      noise_dist = params$noise_dist,
      noise_scale = params$noise_scale,
      noise_df = params$noise_df,
      regime_params = params$regime_params
    )

    all_paths[[i]] <- path_data$returns
    all_params[[i]] <- params

    if (i %% 1000 == 0) {
      message(sprintf("Generated %d/%d paths", i, n_paths))
    }
  }

  result <- list(
    paths = all_paths,
    parameters = all_params,
    horizon = horizon,
    frequency = frequency,
    n_paths = n_paths,
    generation_date = Sys.time()
  )

  class(result) <- "diverse_sv_paths"

  return(result)
}

# Summary method for the generated dataset
summary.diverse_sv_paths <- function(object, ...) {
  cat("Diverse Stochastic Volatility Paths Dataset\n")
  cat("===========================================\n")
  cat(sprintf("Number of paths: %d\n", object$n_paths))
  cat(sprintf("Horizon per path: %d days\n", object$horizon))
  cat(sprintf("Generation date: %s\n", object$generation_date))
  cat(sprintf("Total observations: %d\n", object$n_paths * object$horizon))

  # Sample statistics
  sample_paths <- sample(1:object$n_paths, min(100, object$n_paths))
  returns <- unlist(lapply(object$paths[sample_paths], function(x) x))

  cat("\nSummary statistics (sample):\n")
  cat(sprintf("Mean return: %.6f\n", mean(returns)))
  cat(sprintf("Return SD: %.6f\n", sd(returns)))
  cat(sprintf("Skewness: %.3f\n", moments::skewness(returns)))
  cat(sprintf("Kurtosis: %.3f\n", moments::kurtosis(returns)))
  cat(sprintf("Min return: %.6f\n", min(returns)))
  cat(sprintf("Max return: %.6f\n", max(returns)))
}
```

```{r}
set.seed(12345)
synthetic_data <- generate_diverse_sv_paths(n_paths = 1000, horizon = 500)
synthetic_paths <- synthetic_data$paths
```

# 1 - out-of-sample benchmarking with GARCH models

```{r 1-out-of-sample-benchmarking}
# Install and load required packages
if (!require("rugarch")) install.packages("rugarch")
if (!require("fGarch")) install.packages("fGarch")
if (!require("forecast")) install.packages("forecast")
library(rugarch)
library(fGarch)
library(forecast)

best_params <- readRDS(file="best_params_with_clustering.rds")
print("Bayesian Optimization Parameters:")
print(best_params$best_param)

params <- list()
params$nb_hidden <- floor(best_params$best_param[1])
params$lags <- floor(best_params$best_param[2])
params$lambda_1 <- 10**best_params$best_param[3]
params$lambda_2 <- 10**best_params$best_param[4]
params$centers <- floor(best_params$best_param[6])
print("Transformed Parameters:")
print(params)

# Split data into train (90%) and test (10%)
n_series <- ncol(EuStockMarkets)
stock_data <- tail(EuStockMarkets, 501)
returns_stock_data <- diff(log(stock_data))
print(paste("Data dimensions:", dim(returns_stock_data)[1], "x", dim(returns_stock_data)[2]))

splitted_returns_stock_data <- misc::splitts(returns_stock_data, split_prob = 0.8)
n_test <- length(splitted_returns_stock_data$testing)

# Initialize comprehensive results storage
results <- list()

set.seed(123)

#pb <- utils::txtProgressBar(min = 0, max = ncol(EuStockMarkets), style = 3)

for (i in seq_len(ncol(EuStockMarkets))) {
  series_name <- colnames(EuStockMarkets)[i]
  train <- splitted_returns_stock_data$training[, i]
  test <- splitted_returns_stock_data$testing[, i]
  
  # Store forecasts for each method
  forecasts <- list()
  metrics <- list()
  
  # --- ridge2f (Transfer Learning Approach) ---
  train_mean <- mean(train)
  train_sd <- sd(train)
  scaled_train <- (train - train_mean) / train_sd
  
  fit_ridge <- try(ahead::ridge2f(
    y = scaled_train,
    h = length(test),
    nb_hidden = params$nb_hidden,
    lags = min(params$lags, length(scaled_train) - 1L),
    lambda_1 = params$lambda_1,
    lambda_2 = params$lambda_2,
    centers = params$centers,
    level = 95,
    B = 250,
    type_pi = "movingblockbootstrap",
    show_progress = FALSE, 
  ), silent = TRUE)
  
  if (!inherits(fit_ridge, "try-error")) {
    rescaled_mean <- fit_ridge$mean * train_sd + train_mean
    rescaled_lower <- fit_ridge$lower * train_sd + train_mean
    rescaled_upper <- fit_ridge$upper * train_sd + train_mean
    
    forecasts$ridge2 <- rescaled_mean
    
    # Calculate comprehensive metrics
    metrics$ridge2 <- list(
      winkler = misc::winkler_score(test, rescaled_lower, rescaled_upper, 95),
      coverage = mean((rescaled_lower <= test) & (test <= rescaled_upper)) * 100,
      interval_width = mean(rescaled_upper - rescaled_lower)
    )
  }
  
  # --- GARCH using rugarch ---
  garch_spec <- ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
    mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
    distribution.model = "norm"
  )
  
  garch_fit <- try(ugarchfit(
    spec = garch_spec,
    data = train,
    solver = "hybrid"
  ), silent = TRUE)
  
  if (!inherits(garch_fit, "try-error")) {
    garch_forecast <- ugarchforecast(garch_fit, n.ahead = length(test))
    garch_mean <- as.numeric(fitted(garch_forecast))
    garch_sigma <- as.numeric(sigma(garch_forecast))
    
    z_value <- qnorm(0.975)
    garch_lower <- garch_mean - z_value * garch_sigma
    garch_upper <- garch_mean + z_value * garch_sigma
    
    forecasts$rugarch <- garch_mean
    
    metrics$rugarch <- list(
      winkler = misc::winkler_score(test, garch_lower, garch_upper, 95),
      coverage = mean((garch_lower <= test) & (test <= garch_upper)) * 100,
      interval_width = mean(garch_upper - garch_lower)
    )
  }
  
  # --- GARCH using fGarch ---
  fgarch_fit <- try(garchFit(
    formula = ~ garch(1, 1),
    data = train,
    include.mean = FALSE,
    trace = FALSE,
    cond.dist = "norm"
  ), silent = TRUE)
  
  if (!inherits(fgarch_fit, "try-error")) {
    fgarch_forecast <- predict(fgarch_fit, n.ahead = length(test))
    fgarch_mean <- rep(0, length(test))  # fGarch assumes zero mean
    fgarch_sigma <- fgarch_forecast$standardDeviation
    
    z_value <- qnorm(0.975)
    fgarch_lower <- -z_value * fgarch_sigma
    fgarch_upper <- z_value * fgarch_sigma
    
    forecasts$fgarch <- fgarch_mean
    
    metrics$fgarch <- list(
      winkler = misc::winkler_score(test, fgarch_lower, fgarch_upper, 95),
      coverage = mean((fgarch_lower <= test) & (test <= fgarch_upper)) * 100,
      interval_width = mean(fgarch_upper - fgarch_lower)
    )
  }
  
  # Store results for this series
  results[[series_name]] <- list(forecasts = forecasts, metrics = metrics)
  
  #utils::setTxtProgressBar(pb, i)
}
#close(pb)

# Create comprehensive summary table
summary_table <- data.frame()
for (series_name in names(results)) {
  for (method in names(results[[series_name]]$metrics)) {
    metrics <- results[[series_name]]$metrics[[method]]
    summary_table <- rbind(summary_table, data.frame(
      Series = series_name,
      Method = method,
      Winkler = metrics$winkler,
      Coverage = metrics$coverage,
      Interval_Width = metrics$interval_width
    ))
  }
}

print(summary_table)



# Median performance across all series
avg_performance <- aggregate(. ~ Method, data = summary_table[, -1], median)
print("\n=== MEDIAN PERFORMANCE ACROSS ALL SERIES ===")
print(avg_performance)

# Statistical significance testing
cat("\n=== STATISTICAL COMPARISON ===\n")
methods <- unique(summary_table$Method)
for (metric in c("Winkler", "Coverage")) {
  cat(paste("\n", metric, "comparison:\n"))
  for (i in 1:(length(methods)-1)) {
    for (j in (i+1):length(methods)) {
      m1 <- summary_table[summary_table$Method == methods[i], metric]
      m2 <- summary_table[summary_table$Method == methods[j], metric]
      test_result <- t.test(m1, m2, paired = TRUE)
      cat(sprintf("%s vs %s: p-value = %.4f%s\n",
                  methods[i], methods[j], test_result$p.value,
                  ifelse(test_result$p.value < 0.05, " *", "")))
    }
  }
}
```

# 2 - out-of-sample benchmarking with GARCH models (continued)

```{r 2-additional-benchmark}
# Install and load necessary packages
library(quantmod)
library(zoo)

# List of ticker symbols for the CAC40 (example)
# You can replace this with the real tickers from Wikipedia or other sources
cac40_tickers <- c("AC.PA", "AI.PA", "AIR.PA", "ATO.PA", "BNP.PA", "CAP.PA", "CS.PA", "ENGI.PA", "GLE.PA", "KER.PA")

# Fetch the stock data for each ticker and calculate log-returns
log_returns_list <- list()
date_ <- as.Date("2025-09-05")

for (ticker in cac40_tickers) {
  # Get the stock data for the ticker
  getSymbols(ticker, src = "yahoo", from = date_ - 500, to = date_, auto.assign = TRUE)

  # Retrieve the Adjusted Close Prices
  stock_prices <- Cl(get(ticker))

  # Calculate log-returns
  log_returns <- diff(log(stock_prices))

  # Linear interpolation for missing data (weekends)
  log_returns_interpolated <- na.approx(log_returns)

  # Store the log-returns in the list
  log_returns_list[[ticker]] <- log_returns_interpolated
}

# Convert the list of log-returns into a data frame or matrix
log_returns_matrix <- do.call(cbind, log_returns_list)

# Convert the matrix to a ts object (time series)
log_returns_ts <- ts(log_returns_matrix, start = c(2023, 1), frequency = 252)  # Assuming 252 trading days per year

# View the first few rows of the log-returns time series
#head(log_returns_ts)

# Install and load required packages
if (!require("rugarch")) install.packages("rugarch")
if (!require("fGarch")) install.packages("fGarch")
if (!require("forecast")) install.packages("forecast")
library(rugarch)
library(fGarch)
library(forecast)

best_params <- readRDS(file="best_params_with_clustering.rds")
print("Bayesian Optimization Parameters:")
print(best_params$best_param)

params <- list()
params$nb_hidden <- floor(best_params$best_param[1])
params$lags <- floor(best_params$best_param[2])
params$lambda_1 <- 10**best_params$best_param[3]
params$lambda_2 <- 10**best_params$best_param[4]
params$centers <- floor(best_params$best_param[6])
print("Transformed Parameters:")
print(params)

# Split data into train (90%) and test (10%)
n_series <- ncol(log_returns_ts)
returns_stock_data <- log_returns_ts
print(paste("Data dimensions:", dim(returns_stock_data)[1], "x", dim(returns_stock_data)[2]))

splitted_returns_stock_data <- misc::splitts(returns_stock_data, split_prob = 0.8)
n_test <- length(splitted_returns_stock_data$testing)

# Initialize comprehensive results storage
results <- list()

set.seed(123)

pb <- utils::txtProgressBar(min = 0, max = ncol(log_returns_ts), style = 3)

for (i in seq_len(ncol(log_returns_ts))) {
  series_name <- colnames(log_returns_ts)[i]
  train <- splitted_returns_stock_data$training[, i]
  test <- splitted_returns_stock_data$testing[, i]

  # Store forecasts for each method
  forecasts <- list()
  metrics <- list()

  # --- ridge2f (Transfer Learning Approach) ---
  train_mean <- mean(train)
  train_sd <- sd(train)
  scaled_train <- (train - train_mean) / train_sd

  fit_ridge <- try(ahead::ridge2f(
    y = scaled_train,
    h = length(test),
    nb_hidden = params$nb_hidden,
    lags = min(params$lags, length(scaled_train) - 1L),
    lambda_1 = params$lambda_1,
    lambda_2 = params$lambda_2,
    centers = params$centers,
    level = 95,
    B = 250,
    type_pi = "movingblockbootstrap",
    show_progress = FALSE,
  ), silent = TRUE)

  if (!inherits(fit_ridge, "try-error")) {
    rescaled_mean <- fit_ridge$mean * train_sd + train_mean
    rescaled_lower <- fit_ridge$lower * train_sd + train_mean
    rescaled_upper <- fit_ridge$upper * train_sd + train_mean

    forecasts$ridge2 <- rescaled_mean

    # Calculate comprehensive metrics
    metrics$ridge2 <- list(
      winkler = misc::winkler_score(test, rescaled_lower, rescaled_upper, 95),
      coverage = mean((rescaled_lower <= test) & (test <= rescaled_upper)) * 100,
      interval_width = mean(rescaled_upper - rescaled_lower)
    )
  }

  # --- GARCH using rugarch ---
  garch_spec <- ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
    mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
    distribution.model = "norm"
  )

  garch_fit <- try(ugarchfit(
    spec = garch_spec,
    data = train,
    solver = "hybrid"
  ), silent = TRUE)

  if (!inherits(garch_fit, "try-error")) {
    garch_forecast <- ugarchforecast(garch_fit, n.ahead = length(test))
    garch_mean <- as.numeric(fitted(garch_forecast))
    garch_sigma <- as.numeric(sigma(garch_forecast))

    z_value <- qnorm(0.975)
    garch_lower <- garch_mean - z_value * garch_sigma
    garch_upper <- garch_mean + z_value * garch_sigma

    forecasts$rugarch <- garch_mean

    metrics$rugarch <- list(
      winkler = misc::winkler_score(test, garch_lower, garch_upper, 95),
      coverage = mean((garch_lower <= test) & (test <= garch_upper)) * 100,
      interval_width = mean(garch_upper - garch_lower)
    )
  }

  # --- GARCH using fGarch ---
  fgarch_fit <- try(garchFit(
    formula = ~ garch(1, 1),
    data = train,
    include.mean = FALSE,
    trace = FALSE,
    cond.dist = "norm"
  ), silent = TRUE)

  if (!inherits(fgarch_fit, "try-error")) {
    fgarch_forecast <- predict(fgarch_fit, n.ahead = length(test))
    fgarch_mean <- rep(0, length(test))  # fGarch assumes zero mean
    fgarch_sigma <- fgarch_forecast$standardDeviation

    z_value <- qnorm(0.975)
    fgarch_lower <- -z_value * fgarch_sigma
    fgarch_upper <- z_value * fgarch_sigma

    forecasts$fgarch <- fgarch_mean

    metrics$fgarch <- list(
      winkler = misc::winkler_score(test, fgarch_lower, fgarch_upper, 95),
      coverage = mean((fgarch_lower <= test) & (test <= fgarch_upper)) * 100,
      interval_width = mean(fgarch_upper - fgarch_lower)
    )
  }

  # Store results for this series
  results[[series_name]] <- list(forecasts = forecasts, metrics = metrics)

  utils::setTxtProgressBar(pb, i)
}
close(pb)

# Create comprehensive summary table
summary_table <- data.frame()
for (series_name in names(results)) {
  for (method in names(results[[series_name]]$metrics)) {
    metrics <- results[[series_name]]$metrics[[method]]
    summary_table <- rbind(summary_table, data.frame(
      Series = series_name,
      Method = method,
      Winkler = metrics$winkler,
      Coverage = metrics$coverage,
      Interval_Width = metrics$interval_width
    ))
  }
}

print(summary_table)



# Median performance across all series
avg_performance <- aggregate(. ~ Method, data = summary_table[, -1], median)
print("\n=== MEDIAN PERFORMANCE ACROSS ALL SERIES ===")
print(avg_performance)

# Statistical significance testing
cat("\n=== STATISTICAL COMPARISON ===\n")
methods <- unique(summary_table$Method)
for (metric in c("Winkler", "Coverage")) {
  cat(paste("\n", metric, "comparison:\n"))
  for (i in 1:(length(methods)-1)) {
    for (j in (i+1):length(methods)) {
      m1 <- summary_table[summary_table$Method == methods[i], metric]
      m2 <- summary_table[summary_table$Method == methods[j], metric]
      test_result <- t.test(m1, m2, paired = TRUE)
      cat(sprintf("%s vs %s: p-value = %.4f%s\n",
                  methods[i], methods[j], test_result$p.value,
                  ifelse(test_result$p.value < 0.05, " *", "")))
    }
  }
}
```

# 3 - Viz 

# ============================================================================
# R ggplot2 Visualizations for Transfer Learning Stock Returns Analysis
# Add this section to your existing R Markdown document
# ============================================================================

```{r visualization-setup, message=FALSE, warning=FALSE}
# Load additional packages for visualization
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("dplyr")) install.packages("dplyr")
if (!require("tidyr")) install.packages("tidyr")
if (!require("patchwork")) install.packages("patchwork")
if (!require("ggridges")) install.packages("ggridges")
if (!require("viridis")) install.packages("viridis")
if (!require("plotly")) install.packages("plotly")
if (!require("gridExtra")) install.packages("gridExtra")

library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
library(ggridges)
library(viridis)
library(plotly)
library(gridExtra)

# Set ggplot theme
theme_set(theme_minimal() + 
          theme(
            plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 12, hjust = 0.5),
            legend.position = "bottom",
            panel.grid.minor = element_blank(),
            strip.text = element_text(face = "bold")
          ))

# Custom color palette
method_colors <- c("ridge2f" = "#e74c3c", "rugarch" = "#3498db", "fGarch" = "#f39c12")
```

# 3 - Comprehensive Visualization Analysis

```{r}
library(tidyverse)
library(purrr)
```

```{r create-sample-data}
# Create sample data based on your analysis results
# (Replace this with your actual results data)
set.seed(42)

# Sample performance data
performance_data <- expand_grid(
  Series = c("DAX", "SMI", "CAC", "FTSE", "AC.PA", "AI.PA", "AIR.PA", "ATO.PA", "BNP.PA", "CAP.PA"),
  Method = c("ridge2f", "rugarch", "fGarch")
) %>%
  mutate(
    Winkler = case_when(
      Method == "ridge2f" ~ runif(n(), 0.015, 0.022),
      Method == "rugarch" ~ runif(n(), 0.018, 0.025),
      Method == "fGarch" ~ runif(n(), 0.017, 0.024)
    ),
    Coverage = case_when(
      Method == "ridge2f" ~ runif(n(), 92, 96),
      Method == "rugarch" ~ runif(n(), 89, 93),
      Method == "fGarch" ~ runif(n(), 87, 91)
    ),
    Interval_Width = case_when(
      Method == "ridge2f" ~ runif(n(), 0.065, 0.085),
      Method == "rugarch" ~ runif(n(), 0.075, 0.095),
      Method == "fGarch" ~ runif(n(), 0.070, 0.090)
    )
  )

# Sample synthetic data paths
synthetic_paths_viz <- map_dfr(1:5, ~{
  tibble(
    Path = paste("Path", .x),
    Time = 1:200,
    Returns = cumsum(rnorm(200, 0, 0.02)),
    Volatility = abs(rnorm(200, 0.02, 0.01)),
    Regime = sample(c("Low Vol", "High Vol"), 200, replace = TRUE, prob = c(0.7, 0.3))
  )
})
```

```{r plot1-performance-comparison, fig.width=12, fig.height=8}
# 1. Performance Comparison Across Methods
p1_winkler <- performance_data %>%
  ggplot(aes(x = Series, y = Winkler, fill = Method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = method_colors) +
  labs(title = "Winkler Score Comparison", 
       subtitle = "Lower is Better",
       x = "Stock Series", y = "Winkler Score") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p1_coverage <- performance_data %>%
  ggplot(aes(x = Series, y = Coverage, fill = Method)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = method_colors) +
  geom_hline(yintercept = 95, linetype = "dashed", color = "red", alpha = 0.7) +
  labs(title = "Coverage Comparison", 
       subtitle = "Target: 95% (dashed line)",
       x = "Stock Series", y = "Coverage (%)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine plots
performance_combined <- p1_winkler / p1_coverage
performance_combined + plot_annotation(
  title = "Method Performance Comparison",
  theme = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
)
```

```{r plot2-boxplot-performance, fig.width=10, fig.height=6}
# 2. Box Plot Performance Distribution
performance_long <- performance_data %>%
  pivot_longer(cols = c(Winkler, Coverage, Interval_Width), 
               names_to = "Metric", values_to = "Value")

performance_long %>%
  ggplot(aes(x = Method, y = Value, fill = Method)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.6) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  facet_wrap(~Metric, scales = "free_y") +
  scale_fill_manual(values = method_colors) +
  labs(title = "Performance Distribution Across All Series",
       subtitle = "Boxplots with Individual Data Points",
       x = "Method", y = "Metric Value") +
  theme(legend.position = "none")
```

```{r plot3-synthetic-paths, fig.width=12, fig.height=8}
# 3. Synthetic Stock Returns Visualization
p3_returns <- synthetic_paths_viz %>%
  ggplot(aes(x = Time, y = Returns, color = Path)) +
  geom_line(alpha = 0.8, size = 0.8) +
  scale_color_viridis_d() +
  labs(title = "Synthetic Stock Return Paths",
       subtitle = "Cumulative Returns from Stochastic Volatility Models",
       x = "Time", y = "Cumulative Returns")

p3_volatility <- synthetic_paths_viz %>%
  ggplot(aes(x = Time, y = Volatility, color = Path)) +
  geom_line(alpha = 0.8, size = 0.8) +
  scale_color_viridis_d() +
  labs(title = "Volatility Evolution",
       x = "Time", y = "Volatility")

# Combine plots
(p3_returns / p3_volatility) + plot_layout(guides = "collect")
```

```{r plot4-regime-switching, fig.width=12, fig.height=6}
# 4. Regime Switching Visualization
regime_data <- synthetic_paths_viz %>%
  filter(Path == "Path 1") %>%
  mutate(
    Regime_Binary = ifelse(Regime == "High Vol", 1, 0),
    Return_Daily = c(0, diff(Returns))
  )

p4_regime <- ggplot(regime_data, aes(x = Time)) +
  geom_ribbon(aes(ymin = -0.1, ymax = 0.1, fill = Regime), alpha = 0.3) +
  geom_line(aes(y = Return_Daily), color = "black", alpha = 0.7) +
  scale_fill_manual(values = c("Low Vol" = "#3498db", "High Vol" = "#e74c3c")) +
  labs(title = "Volatility Regime Switching with Daily Returns",
       subtitle = "Shaded areas represent volatility regimes",
       x = "Time", y = "Daily Returns", fill = "Regime") +
  theme(legend.position = "bottom")

print(p4_regime)
```

```{r plot5-heatmap-coverage, fig.width=10, fig.height=6}
# 5. Coverage Heatmap
coverage_matrix <- performance_data %>%
  select(Series, Method, Coverage) %>%
  pivot_wider(names_from = Method, values_from = Coverage) %>%
  column_to_rownames("Series") %>%
  as.matrix()

# Create heatmap data
heatmap_data <- performance_data %>%
  select(Series, Method, Coverage) %>%
  mutate(
    Coverage_Category = case_when(
      Coverage >= 93 ~ "Excellent (â‰¥93%)",
      Coverage >= 90 ~ "Good (90-93%)",
      TRUE ~ "Poor (<90%)"
    )
  )

p5_heatmap <- heatmap_data %>%
  ggplot(aes(x = Method, y = Series, fill = Coverage)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = sprintf("%.1f%%", Coverage)), color = "white", fontface = "bold") +
  scale_fill_gradient2(low = "#e74c3c", mid = "#f39c12", high = "#27ae60",
                       midpoint = 91, name = "Coverage (%)") +
  labs(title = "Coverage Heatmap Across Series and Methods",
       subtitle = "Values shown in each cell",
       x = "Method", y = "Stock Series") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p5_heatmap)
```

```{r plot6-radar-chart, fig.width=10, fig.height=8}
# 6. Performance Radar Chart (using manual approach)
# Create radar chart data
radar_data <- tibble(
  Method = rep(c("ridge2f", "rugarch", "fGarch"), each = 5),
  Metric = rep(c("Accuracy", "Coverage", "Robustness", "Speed", "Stability"), 3),
  Score = c(
    # ridge2f
    85, 94, 88, 75, 92,
    # rugarch  
    78, 91, 85, 90, 87,
    # fGarch
    75, 89, 82, 85, 84
  )
) %>%
  # Convert to polar coordinates for radar effect
  mutate(
    angle = rep(seq(0, 2*pi, length.out = 6)[1:5], 3),
    x = Score * cos(angle),
    y = Score * sin(angle)
  )

# Create radar-like plot
p6_radar <- radar_data %>%
  ggplot(aes(x = x, y = y, color = Method, fill = Method)) +
  geom_polygon(alpha = 0.2) +
  geom_point(size = 3) +
  scale_color_manual(values = method_colors) +
  scale_fill_manual(values = method_colors) +
  coord_fixed() +
  labs(title = "Multi-dimensional Performance Comparison",
       subtitle = "Radar Chart: Higher values are better") +
  theme_void() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )

print(p6_radar)
```

```{r plot7-density-ridges, fig.width=10, fig.height=8}
# 7. Distribution Analysis using Ridge Plots
# Generate sample return distributions for each method
set.seed(123)
return_distributions <- map_dfr(1:100, ~{
  tibble(
    Simulation = .x,
    ridge2f = rnorm(252, 0.0001, 0.018),
    rugarch = rnorm(252, 0.0001, 0.022),
    fGarch = rnorm(252, 0.0001, 0.020)
  )
}) %>%
  pivot_longer(cols = -Simulation, names_to = "Method", values_to = "Returns")

p7_density <- return_distributions %>%
  ggplot(aes(x = Returns, y = Method, fill = Method)) +
  geom_density_ridges(alpha = 0.7, scale = 0.9) +
  scale_fill_manual(values = method_colors) +
  labs(title = "Return Distribution Comparison",
       subtitle = "Density ridges showing distribution shapes",
       x = "Daily Returns", y = "Method") +
  theme(legend.position = "none")

print(p7_density)
```

```{r plot8-time-series-forecast, fig.width=12, fig.height=8}
# 8. Forecast Visualization with Confidence Intervals
# Create sample forecast data
set.seed(456)
n_forecast <- 50
forecast_data <- tibble(
  Time = 1:(100 + n_forecast),
  Type = c(rep("Historical", 100), rep("Forecast", n_forecast)),
  Actual = c(cumsum(rnorm(100, 0, 0.02)), rep(NA, n_forecast)),
  ridge2f_mean = c(rep(NA, 100), cumsum(rnorm(n_forecast, 0.0002, 0.015))),
  ridge2f_lower = c(rep(NA, 100), cumsum(rnorm(n_forecast, -0.01, 0.015))),
  ridge2f_upper = c(rep(NA, 100), cumsum(rnorm(n_forecast, 0.01, 0.015))),
  rugarch_mean = c(rep(NA, 100), cumsum(rnorm(n_forecast, 0.0001, 0.018))),
  rugarch_lower = c(rep(NA, 100), cumsum(rnorm(n_forecast, -0.012, 0.018))),
  rugarch_upper = c(rep(NA, 100), cumsum(rnorm(n_forecast, 0.012, 0.018)))
) %>%
  # Ensure continuity
  mutate(
    ridge2f_mean = ifelse(Time == 101, lag(Actual), ridge2f_mean),
    rugarch_mean = ifelse(Time == 101, lag(Actual), rugarch_mean)
  )

p8_forecast <- forecast_data %>%
  ggplot(aes(x = Time)) +
  # Historical data
  geom_line(aes(y = Actual), color = "black", size = 1, alpha = 0.8) +
  # ridge2f forecast
  geom_ribbon(aes(ymin = ridge2f_lower, ymax = ridge2f_upper), 
              fill = method_colors["ridge2f"], alpha = 0.3) +
  geom_line(aes(y = ridge2f_mean), color = method_colors["ridge2f"], size = 1.2) +
  # rugarch forecast
  geom_ribbon(aes(ymin = rugarch_lower, ymax = rugarch_upper), 
              fill = method_colors["rugarch"], alpha = 0.2) +
  geom_line(aes(y = rugarch_mean), color = method_colors["rugarch"], size = 1.2, linetype = "dashed") +
  geom_vline(xintercept = 100, linetype = "dotted", alpha = 0.7) +
  labs(title = "Out-of-Sample Forecasting Comparison",
       subtitle = "Historical data (black) vs Forecasts with confidence intervals",
       x = "Time", y = "Cumulative Returns") +
  annotate("text", x = 75, y = max(forecast_data$Actual, na.rm = TRUE), 
           label = "Historical", color = "black", fontface = "bold") +
  annotate("text", x = 125, y = max(forecast_data$ridge2f_mean, na.rm = TRUE), 
           label = "ridge2f", color = method_colors["ridge2f"], fontface = "bold") +
  annotate("text", x = 125, y = max(forecast_data$rugarch_mean, na.rm = TRUE) - 0.05, 
           label = "rugarch", color = method_colors["rugarch"], fontface = "bold")

print(p8_forecast)
```


```{r plot10-interactive-plotly, fig.width=12, fig.height=8, eval=FALSE}
# 10. Interactive Performance Dashboard
library(plotly)

# Create interactive performance plot
p10_interactive <- performance_data %>%
  plot_ly(x = ~Series, y = ~Winkler, color = ~Method, colors = method_colors,
          type = 'scatter', mode = 'markers+lines', 
          hovertemplate = "<b>%{fullData.name}</b><br>" +
                         "Series: %{x}<br>" +
                         "Winkler Score: %{y:.4f}<br>" +
                         "Coverage: %{customdata[0]:.1f}%<br>" +
                         "<extra></extra>",
          customdata = ~matrix(Coverage, ncol = 1)) %>%
  layout(title = list(text = "Interactive Performance Dashboard", 
                     font = list(size = 16)),
         xaxis = list(title = "Stock Series"),
         yaxis = list(title = "Winkler Score"),
         hovermode = 'closest')

p10_interactive
```

```{r summary-stats-table, results='asis'}
# Summary Statistics Table
summary_stats <- performance_data %>%
  group_by(Method) %>%
  summarise(
    `Mean Winkler` = round(mean(Winkler), 4),
    `SD Winkler` = round(sd(Winkler), 4),
    `Mean Coverage` = round(mean(Coverage), 1),
    `SD Coverage` = round(sd(Coverage), 1),
    `Mean Interval Width` = round(mean(Interval_Width), 4),
    `SD Interval Width` = round(sd(Interval_Width), 4),
    .groups = 'drop'
  )

knitr::kable(summary_stats, 
             caption = "Summary Statistics by Method",
             format = "html") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```
